{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical\n",
    "# imports\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import mediapipe as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sequences and labels from disk\n",
    "\n",
    "sequences = np.load('../sequences.npy')\n",
    "labels = np.load('../labels.npy')\n",
    "\n",
    "sequences_augmented = np.load('../sequences_augmented.npy')\n",
    "labels_augmented = np.load('../labels_augmented.npy')\n",
    "################!!!!!!!!!!!!!!!!!!!!! they are now nparrays instead of lists, so no need to convert them to nparrays again in the code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sequences_augmented\n",
    "y= to_categorical(labels_augmented).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from keras.layers import Dense, LSTM\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import TensorBoard\n",
    "\n",
    "log_dir = os.path.join('Logs')\n",
    "tb_callback = TensorBoard(log_dir=log_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Steve\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 67ms/step - accuracy: 0.0233 - loss: 6.3630 - val_accuracy: 0.0354 - val_loss: 4.0727\n",
      "Epoch 2/100\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 54ms/step - accuracy: 0.0589 - loss: 3.8994 - val_accuracy: 0.0687 - val_loss: 3.6461\n",
      "Epoch 3/100\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 57ms/step - accuracy: 0.0681 - loss: 3.6074 - val_accuracy: 0.0800 - val_loss: 3.6983\n",
      "Epoch 4/100\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 54ms/step - accuracy: 0.0993 - loss: 3.4924 - val_accuracy: 0.1364 - val_loss: 3.3409\n",
      "Epoch 5/100\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 54ms/step - accuracy: 0.1452 - loss: 3.2554 - val_accuracy: 0.1867 - val_loss: 3.1109\n",
      "Epoch 6/100\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 55ms/step - accuracy: 0.1844 - loss: 3.0096 - val_accuracy: 0.2410 - val_loss: 2.8297\n",
      "Epoch 7/100\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 54ms/step - accuracy: 0.2207 - loss: 2.9188 - val_accuracy: 0.2595 - val_loss: 2.7122\n",
      "Epoch 8/100\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 53ms/step - accuracy: 0.2400 - loss: 2.7758 - val_accuracy: 0.2590 - val_loss: 2.6942\n",
      "Epoch 9/100\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 55ms/step - accuracy: 0.2768 - loss: 2.6435 - val_accuracy: 0.2774 - val_loss: 2.5837\n",
      "Epoch 10/100\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 55ms/step - accuracy: 0.3090 - loss: 2.4387 - val_accuracy: 0.3190 - val_loss: 2.4402\n",
      "Epoch 11/100\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 56ms/step - accuracy: 0.3316 - loss: 2.3672 - val_accuracy: 0.4015 - val_loss: 2.1932\n",
      "Epoch 12/100\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 55ms/step - accuracy: 0.3866 - loss: 2.1963 - val_accuracy: 0.4036 - val_loss: 2.1487\n",
      "Epoch 13/100\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 56ms/step - accuracy: 0.4023 - loss: 2.1433 - val_accuracy: 0.4092 - val_loss: 2.0878\n",
      "Epoch 14/100\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 55ms/step - accuracy: 0.4293 - loss: 1.9934 - val_accuracy: 0.4528 - val_loss: 1.9613\n",
      "Epoch 15/100\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 56ms/step - accuracy: 0.4581 - loss: 1.9499 - val_accuracy: 0.5149 - val_loss: 1.7851\n",
      "Epoch 16/100\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 55ms/step - accuracy: 0.5143 - loss: 1.7679 - val_accuracy: 0.4703 - val_loss: 1.8702\n",
      "Epoch 17/100\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 55ms/step - accuracy: 0.5527 - loss: 1.6086 - val_accuracy: 0.5923 - val_loss: 1.4654\n",
      "Epoch 18/100\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 65ms/step - accuracy: 0.5988 - loss: 1.4654 - val_accuracy: 0.6190 - val_loss: 1.4055\n",
      "Epoch 19/100\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 58ms/step - accuracy: 0.6261 - loss: 1.4243 - val_accuracy: 0.5882 - val_loss: 1.4589\n",
      "Epoch 20/100\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 62ms/step - accuracy: 0.6357 - loss: 1.3515 - val_accuracy: 0.6708 - val_loss: 1.2748\n",
      "Epoch 21/100\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 56ms/step - accuracy: 0.6648 - loss: 1.2323 - val_accuracy: 0.7021 - val_loss: 1.1438\n",
      "Epoch 22/100\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 56ms/step - accuracy: 0.6857 - loss: 1.1644 - val_accuracy: 0.6954 - val_loss: 1.1454\n",
      "Epoch 23/100\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 55ms/step - accuracy: 0.6816 - loss: 1.1584 - val_accuracy: 0.7297 - val_loss: 1.0424\n",
      "Epoch 24/100\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 55ms/step - accuracy: 0.7231 - loss: 1.0670 - val_accuracy: 0.7190 - val_loss: 1.0808\n",
      "Epoch 25/100\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 56ms/step - accuracy: 0.7375 - loss: 0.9750 - val_accuracy: 0.7410 - val_loss: 0.9742\n",
      "Epoch 26/100\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 57ms/step - accuracy: 0.7416 - loss: 0.9833 - val_accuracy: 0.7626 - val_loss: 0.9340\n",
      "Epoch 27/100\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 58ms/step - accuracy: 0.7601 - loss: 0.9051 - val_accuracy: 0.7646 - val_loss: 0.9156\n",
      "Epoch 28/100\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 57ms/step - accuracy: 0.7815 - loss: 0.8584 - val_accuracy: 0.7867 - val_loss: 0.8440\n",
      "Epoch 29/100\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 57ms/step - accuracy: 0.7694 - loss: 0.8812 - val_accuracy: 0.7600 - val_loss: 0.9022\n",
      "Epoch 30/100\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 53ms/step - accuracy: 0.7939 - loss: 0.7773 - val_accuracy: 0.8021 - val_loss: 0.7940\n",
      "Epoch 31/100\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 62ms/step - accuracy: 0.8204 - loss: 0.7164 - val_accuracy: 0.7805 - val_loss: 0.8434\n",
      "Epoch 32/100\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 54ms/step - accuracy: 0.7898 - loss: 0.7848 - val_accuracy: 0.8103 - val_loss: 0.7485\n",
      "Epoch 33/100\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 53ms/step - accuracy: 0.8145 - loss: 0.7093 - val_accuracy: 0.7856 - val_loss: 0.8064\n",
      "Epoch 34/100\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 52ms/step - accuracy: 0.8242 - loss: 0.7110 - val_accuracy: 0.8082 - val_loss: 0.7274\n",
      "Epoch 35/100\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 53ms/step - accuracy: 0.8175 - loss: 0.7009 - val_accuracy: 0.8338 - val_loss: 0.6768\n",
      "Epoch 36/100\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 55ms/step - accuracy: 0.8220 - loss: 0.6730 - val_accuracy: 0.8405 - val_loss: 0.6722\n",
      "Epoch 37/100\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 56ms/step - accuracy: 0.8262 - loss: 0.6747 - val_accuracy: 0.8328 - val_loss: 0.6517\n",
      "Epoch 38/100\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 55ms/step - accuracy: 0.8432 - loss: 0.6376 - val_accuracy: 0.8410 - val_loss: 0.6877\n",
      "Epoch 39/100\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 57ms/step - accuracy: 0.8540 - loss: 0.5842 - val_accuracy: 0.8364 - val_loss: 0.6426\n",
      "Epoch 40/100\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 54ms/step - accuracy: 0.8458 - loss: 0.5886 - val_accuracy: 0.8477 - val_loss: 0.6534\n",
      "Epoch 41/100\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 51ms/step - accuracy: 0.8553 - loss: 0.5673 - val_accuracy: 0.8538 - val_loss: 0.6201\n",
      "Epoch 42/100\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 55ms/step - accuracy: 0.8565 - loss: 0.5708 - val_accuracy: 0.8436 - val_loss: 0.6291\n",
      "Epoch 43/100\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 56ms/step - accuracy: 0.8629 - loss: 0.5416 - val_accuracy: 0.8169 - val_loss: 0.7132\n",
      "Epoch 44/100\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 56ms/step - accuracy: 0.8662 - loss: 0.5520 - val_accuracy: 0.8277 - val_loss: 0.6550\n",
      "Epoch 45/100\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 54ms/step - accuracy: 0.8667 - loss: 0.5621 - val_accuracy: 0.8390 - val_loss: 0.6271\n",
      "Epoch 46/100\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 59ms/step - accuracy: 0.8744 - loss: 0.5140 - val_accuracy: 0.8256 - val_loss: 0.6455\n",
      "Epoch 47/100\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 55ms/step - accuracy: 0.8563 - loss: 0.5452 - val_accuracy: 0.8497 - val_loss: 0.6136\n",
      "Epoch 48/100\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 55ms/step - accuracy: 0.8746 - loss: 0.5067 - val_accuracy: 0.8077 - val_loss: 0.7081\n",
      "Epoch 49/100\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 54ms/step - accuracy: 0.8793 - loss: 0.5077 - val_accuracy: 0.8554 - val_loss: 0.5773\n",
      "Epoch 50/100\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 55ms/step - accuracy: 0.8822 - loss: 0.4654 - val_accuracy: 0.8303 - val_loss: 0.6527\n",
      "Epoch 51/100\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 56ms/step - accuracy: 0.8717 - loss: 0.5145 - val_accuracy: 0.8554 - val_loss: 0.5823\n",
      "Epoch 52/100\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 58ms/step - accuracy: 0.8826 - loss: 0.4873 - val_accuracy: 0.8482 - val_loss: 0.5877\n",
      "Epoch 53/100\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 53ms/step - accuracy: 0.8721 - loss: 0.4732 - val_accuracy: 0.8610 - val_loss: 0.5439\n",
      "Epoch 54/100\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 56ms/step - accuracy: 0.9044 - loss: 0.4201 - val_accuracy: 0.8497 - val_loss: 0.5726\n",
      "Epoch 55/100\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 55ms/step - accuracy: 0.9032 - loss: 0.4083 - val_accuracy: 0.8456 - val_loss: 0.5949\n",
      "Epoch 56/100\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 54ms/step - accuracy: 0.8793 - loss: 0.4796 - val_accuracy: 0.8595 - val_loss: 0.5942\n",
      "Epoch 57/100\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 55ms/step - accuracy: 0.8931 - loss: 0.4487 - val_accuracy: 0.8482 - val_loss: 0.6274\n",
      "Epoch 58/100\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 57ms/step - accuracy: 0.8968 - loss: 0.4385 - val_accuracy: 0.8687 - val_loss: 0.5384\n",
      "Epoch 59/100\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 55ms/step - accuracy: 0.9004 - loss: 0.4219 - val_accuracy: 0.8610 - val_loss: 0.5557\n",
      "Epoch 60/100\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 56ms/step - accuracy: 0.8864 - loss: 0.4525 - val_accuracy: 0.8697 - val_loss: 0.5487\n",
      "Epoch 61/100\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 55ms/step - accuracy: 0.9058 - loss: 0.3875 - val_accuracy: 0.8600 - val_loss: 0.5550\n",
      "Epoch 62/100\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 54ms/step - accuracy: 0.9122 - loss: 0.3816 - val_accuracy: 0.8410 - val_loss: 0.6027\n",
      "Epoch 63/100\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 55ms/step - accuracy: 0.8942 - loss: 0.4420 - val_accuracy: 0.8718 - val_loss: 0.5161\n",
      "Epoch 64/100\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 56ms/step - accuracy: 0.9210 - loss: 0.3707 - val_accuracy: 0.8585 - val_loss: 0.5638\n",
      "Epoch 65/100\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 55ms/step - accuracy: 0.9079 - loss: 0.3821 - val_accuracy: 0.8441 - val_loss: 0.5879\n",
      "Epoch 66/100\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 56ms/step - accuracy: 0.8924 - loss: 0.4277 - val_accuracy: 0.8554 - val_loss: 0.5463\n",
      "Epoch 67/100\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 56ms/step - accuracy: 0.9112 - loss: 0.3961 - val_accuracy: 0.8713 - val_loss: 0.5494\n",
      "Epoch 68/100\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 55ms/step - accuracy: 0.9079 - loss: 0.3712 - val_accuracy: 0.8549 - val_loss: 0.5565\n",
      "Epoch 69/100\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 56ms/step - accuracy: 0.9143 - loss: 0.3752 - val_accuracy: 0.8369 - val_loss: 0.6010\n",
      "Epoch 70/100\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 61ms/step - accuracy: 0.9191 - loss: 0.3656 - val_accuracy: 0.8728 - val_loss: 0.5187\n",
      "Epoch 71/100\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 58ms/step - accuracy: 0.9028 - loss: 0.3950 - val_accuracy: 0.8621 - val_loss: 0.5291\n",
      "Epoch 72/100\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 60ms/step - accuracy: 0.9176 - loss: 0.3559 - val_accuracy: 0.8636 - val_loss: 0.5498\n",
      "Epoch 73/100\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 54ms/step - accuracy: 0.9118 - loss: 0.3851 - val_accuracy: 0.8579 - val_loss: 0.5725\n",
      "Epoch 74/100\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 52ms/step - accuracy: 0.9057 - loss: 0.3808 - val_accuracy: 0.8785 - val_loss: 0.4983\n",
      "Epoch 75/100\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 56ms/step - accuracy: 0.9139 - loss: 0.3426 - val_accuracy: 0.8667 - val_loss: 0.5293\n",
      "Epoch 76/100\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 54ms/step - accuracy: 0.9382 - loss: 0.3096 - val_accuracy: 0.8672 - val_loss: 0.5188\n",
      "Epoch 77/100\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 55ms/step - accuracy: 0.9215 - loss: 0.3471 - val_accuracy: 0.8708 - val_loss: 0.5375\n",
      "Epoch 78/100\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 54ms/step - accuracy: 0.9134 - loss: 0.3825 - val_accuracy: 0.8749 - val_loss: 0.5277\n",
      "Epoch 79/100\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 56ms/step - accuracy: 0.9022 - loss: 0.3689 - val_accuracy: 0.8708 - val_loss: 0.5184\n",
      "Epoch 80/100\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 62ms/step - accuracy: 0.9277 - loss: 0.3219 - val_accuracy: 0.8595 - val_loss: 0.5340\n",
      "Epoch 81/100\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 57ms/step - accuracy: 0.9299 - loss: 0.3104 - val_accuracy: 0.8862 - val_loss: 0.4554\n",
      "Epoch 82/100\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 55ms/step - accuracy: 0.9275 - loss: 0.3107 - val_accuracy: 0.8549 - val_loss: 0.5758\n",
      "Epoch 83/100\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 55ms/step - accuracy: 0.9161 - loss: 0.3454 - val_accuracy: 0.8667 - val_loss: 0.5371\n",
      "Epoch 84/100\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 54ms/step - accuracy: 0.9291 - loss: 0.3349 - val_accuracy: 0.8426 - val_loss: 0.6006\n",
      "Epoch 85/100\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 55ms/step - accuracy: 0.9219 - loss: 0.3372 - val_accuracy: 0.8487 - val_loss: 0.6096\n",
      "Epoch 86/100\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 55ms/step - accuracy: 0.8964 - loss: 0.4086 - val_accuracy: 0.8615 - val_loss: 0.5652\n",
      "Epoch 87/100\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 57ms/step - accuracy: 0.9342 - loss: 0.2987 - val_accuracy: 0.8621 - val_loss: 0.5895\n",
      "Epoch 88/100\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 56ms/step - accuracy: 0.9178 - loss: 0.3325 - val_accuracy: 0.8662 - val_loss: 0.5318\n",
      "Epoch 89/100\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 55ms/step - accuracy: 0.9386 - loss: 0.3136 - val_accuracy: 0.8759 - val_loss: 0.4923\n",
      "Epoch 90/100\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 55ms/step - accuracy: 0.9282 - loss: 0.3311 - val_accuracy: 0.8800 - val_loss: 0.4998\n",
      "Epoch 91/100\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 59ms/step - accuracy: 0.9441 - loss: 0.2830 - val_accuracy: 0.8513 - val_loss: 0.5731\n",
      "Epoch 92/100\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 71ms/step - accuracy: 0.9376 - loss: 0.2781 - val_accuracy: 0.8810 - val_loss: 0.4650\n",
      "Epoch 93/100\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 57ms/step - accuracy: 0.9301 - loss: 0.3104 - val_accuracy: 0.8385 - val_loss: 0.6276\n",
      "Epoch 94/100\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 59ms/step - accuracy: 0.9279 - loss: 0.3085 - val_accuracy: 0.8564 - val_loss: 0.5685\n",
      "Epoch 95/100\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 62ms/step - accuracy: 0.9411 - loss: 0.3140 - val_accuracy: 0.8738 - val_loss: 0.5071\n",
      "Epoch 96/100\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 60ms/step - accuracy: 0.9450 - loss: 0.2607 - val_accuracy: 0.8733 - val_loss: 0.5091\n",
      "Epoch 97/100\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 59ms/step - accuracy: 0.9292 - loss: 0.3078 - val_accuracy: 0.8718 - val_loss: 0.5167\n",
      "Epoch 98/100\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 66ms/step - accuracy: 0.9319 - loss: 0.2939 - val_accuracy: 0.8738 - val_loss: 0.5018\n",
      "Epoch 99/100\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 58ms/step - accuracy: 0.9387 - loss: 0.2802 - val_accuracy: 0.8631 - val_loss: 0.5197\n",
      "Epoch 100/100\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 53ms/step - accuracy: 0.9294 - loss: 0.3228 - val_accuracy: 0.8708 - val_loss: 0.5359\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8618 - loss: 0.4940\n",
      "Epoch 1/100\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 57ms/step - accuracy: 0.8686 - loss: 0.5278 - val_accuracy: 0.9585 - val_loss: 0.2635\n",
      "Epoch 2/100\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 55ms/step - accuracy: 0.8705 - loss: 0.5338 - val_accuracy: 0.9323 - val_loss: 0.3196\n",
      "Epoch 3/100\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 53ms/step - accuracy: 0.8980 - loss: 0.4278 - val_accuracy: 0.9559 - val_loss: 0.2605\n",
      "Epoch 4/100\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 54ms/step - accuracy: 0.9064 - loss: 0.4200 - val_accuracy: 0.9513 - val_loss: 0.2588\n",
      "Epoch 5/100\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 52ms/step - accuracy: 0.9063 - loss: 0.4029 - val_accuracy: 0.9359 - val_loss: 0.2952\n",
      "Epoch 6/100\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 52ms/step - accuracy: 0.9191 - loss: 0.3727 - val_accuracy: 0.9287 - val_loss: 0.3300\n",
      "Epoch 7/100\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 55ms/step - accuracy: 0.9103 - loss: 0.3850 - val_accuracy: 0.9092 - val_loss: 0.3514\n",
      "Epoch 8/100\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 54ms/step - accuracy: 0.9117 - loss: 0.3744 - val_accuracy: 0.9072 - val_loss: 0.3532\n",
      "Epoch 9/100\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 54ms/step - accuracy: 0.9292 - loss: 0.3195 - val_accuracy: 0.9287 - val_loss: 0.2924\n",
      "Epoch 10/100\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 50ms/step - accuracy: 0.9347 - loss: 0.2989 - val_accuracy: 0.9492 - val_loss: 0.2371\n",
      "Epoch 11/100\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 50ms/step - accuracy: 0.9316 - loss: 0.2947 - val_accuracy: 0.9421 - val_loss: 0.2668\n",
      "Epoch 12/100\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 50ms/step - accuracy: 0.9297 - loss: 0.2966 - val_accuracy: 0.9410 - val_loss: 0.2716\n",
      "Epoch 13/100\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 50ms/step - accuracy: 0.9311 - loss: 0.3177 - val_accuracy: 0.9231 - val_loss: 0.3232\n",
      "Epoch 14/100\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 50ms/step - accuracy: 0.9390 - loss: 0.2929 - val_accuracy: 0.9395 - val_loss: 0.2596\n",
      "Epoch 15/100\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 50ms/step - accuracy: 0.9340 - loss: 0.2836 - val_accuracy: 0.9344 - val_loss: 0.2794\n",
      "Epoch 16/100\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 51ms/step - accuracy: 0.9362 - loss: 0.3029 - val_accuracy: 0.9364 - val_loss: 0.2711\n",
      "Epoch 17/100\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 50ms/step - accuracy: 0.9404 - loss: 0.2789 - val_accuracy: 0.9379 - val_loss: 0.2670\n",
      "Epoch 18/100\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 49ms/step - accuracy: 0.9437 - loss: 0.2532 - val_accuracy: 0.9441 - val_loss: 0.2486\n",
      "Epoch 19/100\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 50ms/step - accuracy: 0.9385 - loss: 0.2751 - val_accuracy: 0.9303 - val_loss: 0.2770\n",
      "Epoch 20/100\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 50ms/step - accuracy: 0.9485 - loss: 0.2546 - val_accuracy: 0.9097 - val_loss: 0.3212\n",
      "Epoch 21/100\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 50ms/step - accuracy: 0.9329 - loss: 0.2856 - val_accuracy: 0.9333 - val_loss: 0.2848\n",
      "Epoch 22/100\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 50ms/step - accuracy: 0.9368 - loss: 0.2725 - val_accuracy: 0.9231 - val_loss: 0.3026\n",
      "Epoch 23/100\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 57ms/step - accuracy: 0.9145 - loss: 0.3378 - val_accuracy: 0.9149 - val_loss: 0.3644\n",
      "Epoch 24/100\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 54ms/step - accuracy: 0.9291 - loss: 0.3253 - val_accuracy: 0.9303 - val_loss: 0.2847\n",
      "Epoch 25/100\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 52ms/step - accuracy: 0.9496 - loss: 0.2614 - val_accuracy: 0.9323 - val_loss: 0.2634\n",
      "Epoch 26/100\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 51ms/step - accuracy: 0.9570 - loss: 0.2271 - val_accuracy: 0.9390 - val_loss: 0.2638\n",
      "Epoch 27/100\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 51ms/step - accuracy: 0.9440 - loss: 0.2641 - val_accuracy: 0.9236 - val_loss: 0.3051\n",
      "Epoch 28/100\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 52ms/step - accuracy: 0.9402 - loss: 0.2745 - val_accuracy: 0.9277 - val_loss: 0.2701\n",
      "Epoch 29/100\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 52ms/step - accuracy: 0.9639 - loss: 0.2071 - val_accuracy: 0.9395 - val_loss: 0.2550\n",
      "Epoch 30/100\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 53ms/step - accuracy: 0.9569 - loss: 0.2289 - val_accuracy: 0.9303 - val_loss: 0.2856\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9162 - loss: 0.3846\n",
      "Epoch 1/100\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 63ms/step - accuracy: 0.9254 - loss: 0.3132 - val_accuracy: 0.9662 - val_loss: 0.2084\n",
      "Epoch 2/100\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 53ms/step - accuracy: 0.9042 - loss: 0.3900 - val_accuracy: 0.9723 - val_loss: 0.1721\n",
      "Epoch 3/100\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 56ms/step - accuracy: 0.9418 - loss: 0.2657 - val_accuracy: 0.9733 - val_loss: 0.1762\n",
      "Epoch 4/100\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 54ms/step - accuracy: 0.9343 - loss: 0.2854 - val_accuracy: 0.9677 - val_loss: 0.1922\n",
      "Epoch 5/100\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 53ms/step - accuracy: 0.9342 - loss: 0.2782 - val_accuracy: 0.9646 - val_loss: 0.2124\n",
      "Epoch 6/100\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 61ms/step - accuracy: 0.9349 - loss: 0.2561 - val_accuracy: 0.9569 - val_loss: 0.2086\n",
      "Epoch 7/100\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 60ms/step - accuracy: 0.9428 - loss: 0.2482 - val_accuracy: 0.9626 - val_loss: 0.1992\n",
      "Epoch 8/100\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 55ms/step - accuracy: 0.9505 - loss: 0.2482 - val_accuracy: 0.9497 - val_loss: 0.2516\n",
      "Epoch 9/100\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 56ms/step - accuracy: 0.9443 - loss: 0.2840 - val_accuracy: 0.9569 - val_loss: 0.2242\n",
      "Epoch 10/100\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 57ms/step - accuracy: 0.9513 - loss: 0.2360 - val_accuracy: 0.9574 - val_loss: 0.1988\n",
      "Epoch 11/100\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 61ms/step - accuracy: 0.9491 - loss: 0.2221 - val_accuracy: 0.9549 - val_loss: 0.2187\n",
      "Epoch 12/100\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 53ms/step - accuracy: 0.9468 - loss: 0.2525 - val_accuracy: 0.9631 - val_loss: 0.1998\n",
      "Epoch 13/100\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 55ms/step - accuracy: 0.9254 - loss: 0.3190 - val_accuracy: 0.9677 - val_loss: 0.2058\n",
      "Epoch 14/100\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 55ms/step - accuracy: 0.9506 - loss: 0.2295 - val_accuracy: 0.9677 - val_loss: 0.1897\n",
      "Epoch 15/100\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 56ms/step - accuracy: 0.9539 - loss: 0.2263 - val_accuracy: 0.9621 - val_loss: 0.1986\n",
      "Epoch 16/100\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 56ms/step - accuracy: 0.9602 - loss: 0.2189 - val_accuracy: 0.9615 - val_loss: 0.1979\n",
      "Epoch 17/100\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 56ms/step - accuracy: 0.9374 - loss: 0.2511 - val_accuracy: 0.9605 - val_loss: 0.2059\n",
      "Epoch 18/100\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 54ms/step - accuracy: 0.9617 - loss: 0.2049 - val_accuracy: 0.9636 - val_loss: 0.1922\n",
      "Epoch 19/100\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 54ms/step - accuracy: 0.9580 - loss: 0.2083 - val_accuracy: 0.9323 - val_loss: 0.2665\n",
      "Epoch 20/100\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 56ms/step - accuracy: 0.9486 - loss: 0.2344 - val_accuracy: 0.9482 - val_loss: 0.2406\n",
      "Epoch 21/100\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 54ms/step - accuracy: 0.9382 - loss: 0.2700 - val_accuracy: 0.9451 - val_loss: 0.2358\n",
      "Epoch 22/100\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 55ms/step - accuracy: 0.9550 - loss: 0.2173 - val_accuracy: 0.9467 - val_loss: 0.2234\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8910 - loss: 0.3979\n",
      "Average Test Accuracy: 89.28%\n"
     ]
    }
   ],
   "source": [
    "# With Regularization\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dropout, Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras.regularizers import l2\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Define the model\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, return_sequences=True, input_shape=(30, 1662), dropout=0.2))  # LSTM layer with dropout\n",
    "model.add(LSTM(128, return_sequences=True, kernel_regularizer=l2(0.01)))  # LSTM layer with L2 regularization\n",
    "model.add(LSTM(128, kernel_regularizer=l2(0.01)))  # LSTM layer with L2 regularization\n",
    "model.add(Dense(65, activation='softmax'))  # Output layer matches the number of classes\n",
    "\n",
    "# Compile the model\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "kf = KFold(n_splits=3, shuffle=True)\n",
    "\n",
    "test_scores = []\n",
    "## Perform k-fold cross-validation\n",
    "for train_index, val_index in kf.split(X_train):\n",
    "    X_train_fold, X_val_fold = X_train[train_index], X_train[val_index]\n",
    "    y_train_fold, y_val_fold = y_train[train_index], y_train[val_index]\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X_train_fold, y_train_fold, epochs=100, validation_data=(X_val_fold, y_val_fold),\n",
    "              callbacks=[EarlyStopping(monitor='val_loss', patience=5)])\n",
    "    \n",
    "    test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "    test_scores.append(test_accuracy)\n",
    "\n",
    "# Print the average test accuracy across all folds\n",
    "print(\"Average Test Accuracy: %.2f%%\" % (sum(test_scores) / len(test_scores) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# Convert the list of test scores into a DataFrame\n",
    "test_scores_df = pd.DataFrame(test_scores, columns=['Test Accuracy'])\n",
    "\n",
    "# Draw a box plot of the test scores\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(data=test_scores_df)\n",
    "plt.title('Box plot of test scores')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "model.save('../actions/actions_triple_lstm.h5')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 37ms/step\n",
      "0.9\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[635,   1],\n",
       "        [  2,  12]],\n",
       "\n",
       "       [[636,   4],\n",
       "        [  6,   4]],\n",
       "\n",
       "       [[629,   5],\n",
       "        [  5,  11]],\n",
       "\n",
       "       [[637,   0],\n",
       "        [  0,  13]],\n",
       "\n",
       "       [[633,   4],\n",
       "        [  2,  11]],\n",
       "\n",
       "       [[639,   0],\n",
       "        [  0,  11]],\n",
       "\n",
       "       [[639,   4],\n",
       "        [  2,   5]],\n",
       "\n",
       "       [[638,   3],\n",
       "        [  0,   9]],\n",
       "\n",
       "       [[643,   1],\n",
       "        [  1,   5]],\n",
       "\n",
       "       [[646,   0],\n",
       "        [  0,   4]],\n",
       "\n",
       "       [[637,   2],\n",
       "        [  3,   8]],\n",
       "\n",
       "       [[641,   1],\n",
       "        [  3,   5]],\n",
       "\n",
       "       [[641,   1],\n",
       "        [  0,   8]],\n",
       "\n",
       "       [[638,   0],\n",
       "        [  2,  10]],\n",
       "\n",
       "       [[636,   3],\n",
       "        [  1,  10]],\n",
       "\n",
       "       [[639,   1],\n",
       "        [  1,   9]],\n",
       "\n",
       "       [[639,   0],\n",
       "        [  0,  11]],\n",
       "\n",
       "       [[638,   2],\n",
       "        [  4,   6]],\n",
       "\n",
       "       [[637,   2],\n",
       "        [  0,  11]],\n",
       "\n",
       "       [[645,   0],\n",
       "        [  1,   4]],\n",
       "\n",
       "       [[643,   0],\n",
       "        [  0,   7]],\n",
       "\n",
       "       [[640,   1],\n",
       "        [  1,   8]],\n",
       "\n",
       "       [[641,   2],\n",
       "        [  0,   7]],\n",
       "\n",
       "       [[640,   0],\n",
       "        [  2,   8]],\n",
       "\n",
       "       [[639,   2],\n",
       "        [  2,   7]],\n",
       "\n",
       "       [[638,   1],\n",
       "        [  0,  11]],\n",
       "\n",
       "       [[638,   1],\n",
       "        [  0,  11]],\n",
       "\n",
       "       [[641,   1],\n",
       "        [  2,   6]],\n",
       "\n",
       "       [[640,   0],\n",
       "        [  0,  10]],\n",
       "\n",
       "       [[636,   0],\n",
       "        [  2,  12]],\n",
       "\n",
       "       [[638,   0],\n",
       "        [  2,  10]],\n",
       "\n",
       "       [[633,   1],\n",
       "        [  0,  16]],\n",
       "\n",
       "       [[636,   0],\n",
       "        [  1,  13]],\n",
       "\n",
       "       [[636,   0],\n",
       "        [  0,  14]],\n",
       "\n",
       "       [[639,   1],\n",
       "        [  2,   8]],\n",
       "\n",
       "       [[647,   0],\n",
       "        [  0,   3]],\n",
       "\n",
       "       [[642,   1],\n",
       "        [  0,   7]],\n",
       "\n",
       "       [[641,   0],\n",
       "        [  1,   8]],\n",
       "\n",
       "       [[635,   1],\n",
       "        [  1,  13]],\n",
       "\n",
       "       [[638,   2],\n",
       "        [  2,   8]],\n",
       "\n",
       "       [[642,   0],\n",
       "        [  0,   8]],\n",
       "\n",
       "       [[642,   1],\n",
       "        [  1,   6]],\n",
       "\n",
       "       [[640,   2],\n",
       "        [  0,   8]],\n",
       "\n",
       "       [[638,   0],\n",
       "        [  1,  11]],\n",
       "\n",
       "       [[641,   0],\n",
       "        [  0,   9]],\n",
       "\n",
       "       [[642,   1],\n",
       "        [  0,   7]],\n",
       "\n",
       "       [[640,   0],\n",
       "        [  1,   9]],\n",
       "\n",
       "       [[641,   1],\n",
       "        [  1,   7]],\n",
       "\n",
       "       [[645,   0],\n",
       "        [  0,   5]],\n",
       "\n",
       "       [[638,   1],\n",
       "        [  1,  10]],\n",
       "\n",
       "       [[637,   0],\n",
       "        [  0,  13]],\n",
       "\n",
       "       [[640,   2],\n",
       "        [  1,   7]],\n",
       "\n",
       "       [[642,   0],\n",
       "        [  0,   8]],\n",
       "\n",
       "       [[636,   0],\n",
       "        [  3,  11]],\n",
       "\n",
       "       [[636,   0],\n",
       "        [  0,  14]],\n",
       "\n",
       "       [[644,   0],\n",
       "        [  0,   6]],\n",
       "\n",
       "       [[638,   0],\n",
       "        [  0,  12]],\n",
       "\n",
       "       [[635,   2],\n",
       "        [  0,  13]],\n",
       "\n",
       "       [[642,   3],\n",
       "        [  2,   3]],\n",
       "\n",
       "       [[640,   0],\n",
       "        [  0,  10]],\n",
       "\n",
       "       [[635,   1],\n",
       "        [  0,  14]],\n",
       "\n",
       "       [[633,   2],\n",
       "        [  1,  14]],\n",
       "\n",
       "       [[642,   0],\n",
       "        [  1,   7]],\n",
       "\n",
       "       [[640,   0],\n",
       "        [  1,   9]],\n",
       "\n",
       "       [[639,   1],\n",
       "        [  0,  10]]], dtype=int64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report, multilabel_confusion_matrix\n",
    "\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "ytrue = np.argmax(y_test, axis=1).tolist()\n",
    "ypred = np.argmax(y_pred, axis=1).tolist()\n",
    "\n",
    "print(accuracy_score(ytrue, ypred))\n",
    "multilabel_confusion_matrix(ytrue, ypred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
